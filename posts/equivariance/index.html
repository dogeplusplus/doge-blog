<!doctype html><html><head><title>Equivariance Properties in Machine Learning</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><link rel=stylesheet href=/css/bootstrap.min.css><link rel=stylesheet href=/css/layouts/main.css><link rel=stylesheet href=/css/navigators/navbar.css><link rel=stylesheet href=/css/plyr.css><link rel=stylesheet href=/css/flag-icon.min.css><link rel=stylesheet href=/google-fonts/Mulish/mulish.css><link rel=stylesheet href=/fontawesome/css/all.min.css><meta property="og:title" content="Equivariance Properties in Machine Learning"><meta property="og:description" content="Testing models for equivariance properties"><meta property="og:type" content="article"><meta property="og:url" content="https://dogeplusplus.github.io/posts/equivariance/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-14T00:00:00+01:00"><meta property="article:modified_time" content="2022-11-14T00:00:00+01:00"><meta name=description content="Testing models for equivariance properties"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-dark.min.css><link rel=stylesheet href=/css/layouts/single.css><link rel=stylesheet href=/css/navigators/sidebar.css><link rel=stylesheet href=/css/style.css></head><body data-spy=scroll data-target=#TableOfContents data-offset=80><div class="container-fluid bg-dimmed wrapper"><nav class="navbar navbar-expand-xl top-navbar final-navbar shadow"><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button onclick=toggleSidebar()>
<span class=navbar-toggler-icon></span></button>
<a class=navbar-brand href=/>dogeplusplus</a>
<button class="navbar-toggler navbar-light" id=toc-toggler type=button onclick=toggleTOC()>
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse lang-selector" id=top-nav-items><ul class="navbar-nav ml-auto"></ul></div></div></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts/ data-filter=all>Posts</a></li><div class=subtree><li><a class=active href=/posts/equivariance/ title="Equivariance Properties in Machine Learning">Equivariance Properties in Machine Learning</a></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/posts/equivariance/hero.png)></div><div class=page-content><div class="author-profile ml-auto align-self-lg-center"><img class=rounded-circle src=/images/github_avatar_huaaace00864fbf71305074b3b3c71f41f_68121_120x120_fit_q75_box.jpeg alt="Author Image"><h5 class=author-name>Albert Chung</h5><p>Monday, November 14, 2022</p></div><div class=title><h1>Equivariance Properties in Machine Learning</h1></div><div class=post-content id=post-content><h3 id=introduction>Introduction</h3><p>When we think about the signs for what is a good model, we first look to notions of accuracy, precision, recall and the like. Evaluating these metrics typically require a ground truth to evaluate against, and is rarely the case that a machine learning practitioner will not have at least a few examples to test on.</p><p>In medical imaging deep learning is applied to make measurements about our bodies such as organ size, which helps us gain understand our health. In order for any measurement to be useful, it should be invariant to small changes like the position of the patient, while sensitive to physiological changes such as the progression of a disease.</p><p>This sameness property is also desired in machine learning, the idea that if we make changes to a model input, the output of the model should change in a predictable manner (or not change). One example is taking a cat picture and flipping it vertically should cause a good classification model to still predict a cat class (invariance). And for a segmentation model the mask of the flipped cat should just be the flipped version of the mask predicted on the unflipped image (equivariance).</p><div style=margin-top:4rem></div><img src=images/group_sam_pic.png class=center><p>Data augmentation is the go-to method for making models robust to these kinds of changes and is an implicit way of achieving this by exposing these types of scenarios (rotations, brightness changes, noise) to the model during the training process.</p><img src=images/augmentation_examples.png width=500 class=center><p>We are often quick to instinctively apply data augmentation, and are happy with the outcome so long is the performance improves on our validation set. It is rarer that we verify if in doing so did the models actually learn to be robust to this kind of noise. I will take a step further by picking an individual image and observe how robust the prediction of the model is to varying levels of data augmentation.</p><h3 id=dataset>Dataset</h3><ul><li>I will be using the Leeds Butterfly Dataset consisting of 832 images in total of 10 types of butterflies.</li></ul><p><div style=margin-top:4rem></div><img src=images/butterfly_example.png width=500 class=center><div style=margin-top:4rem></div></p><ul><li>The masks are binary and do not take into account the type of butterfly, so the task is foreground segmentation of the butterfly</li><li>We resize the images to <code>(256, 256)</code> when training the models, and pad to (384, 384) with black pixels to perform rotations of the mask later on without losing information at the corners of the mask</li><li>The values are normalised with mean <code>(0.485, 0.456, 0.406)</code> and standard deviation <code>(0.229, 0.224, 0.225)</code></li><li>20% of the images will be reserved for validation</li></ul><h3 id=models>Models</h3><p>4 models will be trained in total using 2 sets of data augmentation pipelines and 2 model architectures. These a regular U-Net with Group Normalisation layers with a sigmoid activation layer is applied to the logits to get the foreground probability as there is only one foreground class.</p><p>The second model is a U-Net where the Convolutions are replaced with Group Convolutions that run on group feature maps. More details can be found at the University of Amsterdam&rsquo;s <a href=https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/Geometric_deep_learning/tutorial1_regular_group_convolutions.html>GDL tutorials</a> describing Group Theory in Machine Learning, and there is also the original Cohen/Welling paper on <a href=https://arxiv.org/abs/1602.07576>Group Equivariant Convolutional Networks</a>. We use the cyclic rotation group of order 4 corresponding to 90 degree rotations. This allows the model to be robust to rotation without having to perform data augmentation.</p><p><div style=margin-top:4rem></div><img src=images/equivariance_figure.png class=center><div style=margin-top:4rem></div></p><h4 id=model-parameters>Model Parameters</h4><ul><li>U-Net configurations for both the original and Group U-Net use 4 downsampling blocks with residual connections from the input of the block to its output.</li><li>As the the group convolutions operate on the group features the number of trainable parameters is higher. We halve the number of filters in each of the group convolution layers which brings the number of trainable parameters to around the same.<ul><li>U-Net - <code>[32, 32, 64, 64]</code> filters in the downsampling blocks and the reverse during upsampling</li><li>Group U-Net - <code>[16, 16, 32, 32]</code> filters in the downsampling blocks and the reverse during upsampling</li></ul></li><li>In all networks we perform Group Normalisation on the channels</li></ul><h4 id=augmentation-sets>Augmentation Sets</h4><p>We create a base set of data augmentation using <a href=https://albumentations.ai/>Albumentations</a> which includes just the preprocessing steps such as normalisation, and constant padding around the borders.</p><p>Each model type was additionally trained using a fuller set of augmentations such as rotations of up to 90 degrees, brightness/contrast changes, and color adjustments.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> albumentations <span style=color:#66d9ef>as</span> A
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> albumentations.pytorch.transforms <span style=color:#f92672>import</span> ToTensorV2
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> albumentations.augmentations.geometric.rotate <span style=color:#f92672>import</span> SafeRotate
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> albumentations.augmentations.transforms <span style=color:#f92672>import</span> ColorJitter
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> albumentations.augmentations.transforms <span style=color:#f92672>import</span> RandomBrightnessContrast
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>preprocessing <span style=color:#f92672>=</span> A<span style=color:#f92672>.</span>Compose([
</span></span><span style=display:flex><span>	A<span style=color:#f92672>.</span>PadIfNeeded(width, width, border_mode<span style=color:#f92672>=</span>cv2<span style=color:#f92672>.</span>BORDER_CONSTANT, value<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>),
</span></span><span style=display:flex><span>	A<span style=color:#f92672>.</span>Normalize(),
</span></span><span style=display:flex><span>	ToTensorV2(),
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>full_transform <span style=color:#f92672>=</span> A<span style=color:#f92672>.</span>Compose([
</span></span><span style=display:flex><span>	ColorJitter(),
</span></span><span style=display:flex><span>	RandomBrightnessContrast(),
</span></span><span style=display:flex><span>	SafeRotate(limit<span style=color:#f92672>=</span><span style=color:#ae81ff>90</span>),
</span></span><span style=display:flex><span>	preprocessing,
</span></span><span style=display:flex><span>])
</span></span></code></pre></div><h3 id=training--evaluation>Training & Evaluation</h3><ul><li>We train each of the models for 150 epochs</li><li>The <code>AdamW</code> optimiser is used with an initial learning rate of 1e-3 and a default weight decay of 1e-2</li><li>A batch size of 16 is used during training</li><li>For loss we use the <code>BCEWithLogitsLoss</code></li><li>For accuracy we calculate the Intersection over Union (IoU)</li><li>More implementation details for this experiment can be found <a href=https://github.com/dogeplusplus/group-unet>here</a></li></ul><p>We monitor the training/validation performance over time and look at the final values. The validation dataset undergoes the base preprocessing but with an additional random number of 90 degree rotations to evaluate the robustness of these models to this particular transformation.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> albumentations <span style=color:#66d9ef>as</span> A
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>val_transform <span style=color:#f92672>=</span> A<span style=color:#f92672>.</span>Compose([
</span></span><span style=display:flex><span>	A<span style=color:#f92672>.</span>RandomRotate90(),
</span></span><span style=display:flex><span>	preprocessing,
</span></span><span style=display:flex><span>])
</span></span></code></pre></div><p>In addition, we sample a single image and run an additional test:</p><ol><li>Create a set of images consisting of a varying number of rotations from 0 to 360 degrees</li><li>Run the stack through the model</li><li>Invert the masks back to the original orientation of the base image by rotating the mask by the angle in the opposite direction</li><li>Apply centre crop to remove the black borders introduced during preprocessing</li><li>Calculate the intersection over union between the set of predictions and the single unaltered ground truth, and plot these scores as a function of the rotation angle</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> cv2
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> albumentations <span style=color:#66d9ef>as</span> A
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> albumentations.pytorch.transforms <span style=color:#f92672>import</span> ToTensorV2
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torchvision.transforms.functional <span style=color:#f92672>import</span> rotate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>batched_prediction</span>(
</span></span><span style=display:flex><span>    model: nn<span style=color:#f92672>.</span>Module,
</span></span><span style=display:flex><span>    image: np<span style=color:#f92672>.</span>ndarray,
</span></span><span style=display:flex><span>    angles: List[float],
</span></span><span style=display:flex><span>    pad_size: int <span style=color:#f92672>=</span> <span style=color:#ae81ff>384</span>,
</span></span><span style=display:flex><span>) <span style=color:#f92672>-&gt;</span> torch<span style=color:#f92672>.</span>Tensor:
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> cv2<span style=color:#f92672>.</span>cvtColor(image, cv2<span style=color:#f92672>.</span>COLOR_BGR2RGB)
</span></span><span style=display:flex><span>    h, w, _ <span style=color:#f92672>=</span> image<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Pad and rotate images</span>
</span></span><span style=display:flex><span>    preprocessing <span style=color:#f92672>=</span> A<span style=color:#f92672>.</span>Compose([
</span></span><span style=display:flex><span>        A<span style=color:#f92672>.</span>PadIfNeeded(pad_size, pad_size, border_mode<span style=color:#f92672>=</span>cv2<span style=color:#f92672>.</span>BORDER_CONSTANT, value<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>),
</span></span><span style=display:flex><span>        A<span style=color:#f92672>.</span>Normalize(),
</span></span><span style=display:flex><span>        ToTensorV2(),
</span></span><span style=display:flex><span>    ])
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> preprocessing(image<span style=color:#f92672>=</span>image)[<span style=color:#e6db74>&#34;image&#34;</span>]
</span></span><span style=display:flex><span>    images <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>stack([rotate(image, angle) <span style=color:#66d9ef>for</span> angle <span style=color:#f92672>in</span> angles])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>    predictions <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, images<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>], batch_size):
</span></span><span style=display:flex><span>        pred <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sigmoid(model(images[i:i<span style=color:#f92672>+</span>batch_size])) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>        pred <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>squeeze(pred)
</span></span><span style=display:flex><span>        predictions<span style=color:#f92672>.</span>append(pred)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Invert padding and centre crop</span>
</span></span><span style=display:flex><span>    predictions <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>concat(predictions, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    predictions <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>concat([
</span></span><span style=display:flex><span>        rotate(repeat(pred, <span style=color:#e6db74>&#34;... -&gt; 1 ...&#34;</span>), <span style=color:#f92672>-</span>angle) <span style=color:#66d9ef>for</span> pred, angle <span style=color:#f92672>in</span> zip(predictions, angles)
</span></span><span style=display:flex><span>    ], dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    crop_w <span style=color:#f92672>=</span> (pad_size <span style=color:#f92672>-</span> w) <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>    crop_h <span style=color:#f92672>=</span> (pad_size <span style=color:#f92672>-</span> h) <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>    predictions <span style=color:#f92672>=</span> predictions[:, crop_h:<span style=color:#f92672>-</span>crop_h, crop_w:<span style=color:#f92672>-</span>crop_w]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> predictions
</span></span></code></pre></div><h3 id=results>Results</h3><p>The plain U-Net with little augmentation did best in terms of learning the data distribution that it was presented. Despite having less augmentation and no specialised layers, this model had the highest validation dice score. The difference between the best and worst performing model for this metric is around 0.03. The gap in validation loss is even smaller.</p><ul><li>The original U-Net with minimal preprocessing performed the best in terms of learning its data distribution</li><li>Many of the Group U-Nets did not predict anything for several epochs</li><li>The two models trained with just the base preprocessing saw a drop in performance around epoch 110</li><li>Using the full set of augmentations resulted in models that performed less than their minimal counterparts.</li></ul><h4 id=final-epoch-performance>Final Epoch Performance</h4><table><thead><tr><th>Model</th><th>Data Augmentation</th><th>Train IoU</th><th>Train Loss</th><th>Validation IoU</th><th>Validation Loss</th></tr></thead><tbody><tr><td>Plain U-Net</td><td>Minimal</td><td><strong>0.9404</strong></td><td><strong>0.007263</strong></td><td><strong>0.8678</strong></td><td>0.05585</td></tr><tr><td>Plain U-Net</td><td>Full</td><td>0.8543</td><td>0.0247</td><td>0.8522</td><td>0.05425</td></tr><tr><td>Group U-Net</td><td>Minimal</td><td>0.9076</td><td>0.007864</td><td>0.8563</td><td><strong>0.05154</strong></td></tr><tr><td>Group U-Net</td><td>Full</td><td>0.7927</td><td>0.01696</td><td>0.8332</td><td>0.06099</td></tr></tbody></table><p><img src=images/train_loss.png alt>
<img src=images/train_acc.png alt>
<img src=images/val_loss.png alt>
<img src=images/val_acc.png alt></p><h4 id=rotated-predictions>Rotated Predictions</h4><p>For each of the models we predict on a single image rotated with multiple angles and plot the dice scores for each angle. In this example we see that despite not having the highest IoU, the Group U-Net predictions are more consistent with the angle. Whereas for other models the IoU score can change as much as 0.04 depending on the chosen angle.</p><p>In the Group U-Net Full Augmentation and Plain U-Net minimal augmentation setups it is easy to see that there is a lot of variance with predicting the main body of the butterfly.</p><p>Occasionally the models will segment the border of the image as a butterfly, but I think this is due to my choice of zero padding and the model associating dark pixels with the wings of the butterfly as is the case for a few of the species in this dataset.</p><h5 id=plain-u-net-full-augmentation>Plain U-Net Full Augmentation</h5><img src=images/plain_unet_full.gif class=center><h5 id=plain-u-net-minimal-augmentation>Plain U-Net Minimal Augmentation</h5><img src=images/plain_unet_minimal.gif class=center><h5 id=group-u-net-full-augmentation>Group U-Net Full Augmentation</h5><img src=images/group_unet_full.gif class=center><h5 id=group-u-net-minimal-augmentation>Group U-Net Minimal Augmentation</h5><img src=images/group_unet_minimal.gif class=center><h3 id=conclusion>Conclusion</h3><p>The models trained here have a lot of room for improvement. The key message is that thinking beyond comparisons to a ground truth like equivariance properties can be a useful test in situations where labels are scarce or even impossible to obtain.</p><p>Regardless of how you design your model, one can test explicitly test equivariance by taking an inputs and perturbing it by a range of values, and observing what happens. If your predictions are consistent across a range of perturbations, then that might be one sign of a good model.</p></div><div class="row pl-3 pr-3"><div class="col-md-6 share-buttons"></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/dogeplusplus/dogeplusplus.github.io/edit/main/content/posts/equivariance/index.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Improve this page</a></div></div><hr><div class="row next-prev-navigator"></div><hr></div></div></div><a id=scroll-to-top class=btn><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center pl-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><ul><li><a href=#introduction>Introduction</a></li><li><a href=#dataset>Dataset</a></li><li><a href=#models>Models</a><ul><li><a href=#model-parameters>Model Parameters</a></li><li><a href=#augmentation-sets>Augmentation Sets</a></li></ul></li><li><a href=#training--evaluation>Training & Evaluation</a></li><li><a href=#results>Results</a><ul><li><a href=#final-epoch-performance>Final Epoch Performance</a></li><li><a href=#rotated-predictions>Rotated Predictions</a><ul><li><a href=#plain-u-net-full-augmentation>Plain U-Net Full Augmentation</a></li><li><a href=#plain-u-net-minimal-augmentation>Plain U-Net Minimal Augmentation</a></li><li><a href=#group-u-net-full-augmentation>Group U-Net Full Augmentation</a></li><li><a href=#group-u-net-minimal-augmentation>Group U-Net Minimal Augmentation</a></li></ul></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-left"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://dogeplusplus.github.io#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://dogeplusplus.github.io#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=https://dogeplusplus.github.io#projects>Projects</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:albertswchung[at]gmail[dot]com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>albertswchung[at]gmail[dot]com</span></a></li><li><span><i class="fas fa-phone-alt"></i></span> <span>+44 7919986840</span></li></ul></div></div></div><hr><div class=container><div class="row text-left"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu8376fd15465fef26ffe66b6bcf0ca686_13669_32x0_resize_box_3.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2022 Copyright.</div><div class="col-md-4 text-right"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script type=text/javascript src=/js/jquery-3.4.1.min.js></script>
<script type=text/javascript src=/js/popper.min.js></script>
<script type=text/javascript src=/js/bootstrap.min.js></script>
<script type=text/javascript src=/js/navbar.js></script>
<script type=text/javascript src=/js/plyr.js></script>
<script type=text/javascript src=/js/main.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js></script>
<script src=/js/single.js></script>
<script>hljs.initHighlightingOnLoad()</script></body></html>